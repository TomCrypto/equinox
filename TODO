1. fix the aliasing artifacts of the grid, really not sure where they are coming from...
    -> seems to be bias introduced by the one-photon limit per cell, not entirely sure why yet though;
       -> may be a hardware thing? it will draw point fragments in a specific order? should still be random though, needs to be investigated

2. precompute the visible point paths instead of recomputing them every single photon batch, this will give a huge speedup
  -> this has been done
3. try out some caustic paths on some complex scenes
4. store the squared radius per-pixel instead of the radius, this simplifies some logic and avoids a sqrt...
5. implement more bounces for photons to include additional light paths

How can we handle exponential growth of visible points correctly with interfaces? Should we do a 50/50 split? At the moment the reflected
visible point is used 4% of the time or so
 -> I tried this and it does seem to lead to more consistent results but still not ideal, see if PBR book has something to say about that

Need to implement some kind of heuristic to handle duplicate photons. Maybe we can just update a stencil buffer to count how many photons land in
a given grid, and multiply the photon throughput by that count for any photon inside the corresponding cell; this approximates multiple photons
landing n the same spot.

There's a bug where the image is quite far converged, eventually some pixels start producing NaNs. This seems to happen towards the end of the
process and never at the beginning which suggests something going wrong when the radius is too small? This needs to be investigated...
  -> this is fixed, had to do with the radius being zero and the gather phase producing infinite radiance as a result

Loss of precision in visible points with no photons needs to be investigated.
 -> fixed by upgrading to 32-bit floats, we need to see if we can use 16-bit anyway. Problem is the visible point data is stored such that it
    continually increments, if we store it in a normalized form (e.g. divided by number of SPPM passes) it would work better.

I think the grid artifacting is due to the fact that only one photon can land in each cell in the grid; for sufficiently large cells this shows up
as bias because each photon is constrained to that cell range... not sure though.

How do we deal with large grid cells? Many photons will fall into them, probably from random directions.

We must have a strategy for large grid cells because otherwise we cannot use them for initial passes, meaning our initial approximations will be
too high variance to be useful. At the moment there are two artifacts that arise:

 1) "blocky" appearance on curved surfaces, not sure yet what causes this
 2) "splotches" that appear in early passes; this is likely the result of hash collisions which cause one photon on a surface to be overwritten by
    another distant photon which happened to be mapped to the same cell. This can be mitigated by making the hash table larger, and using less
    photons in initial passes where the photon accuracy needs to be high.

One possible strategy is to simply trace many fewer photons in early passes, but do that with high accuracy. I don't know if this is a good idea
though.

The "blocky" issue "resolves" itself at lower grid cell sizes, but I need to investigate that. The splotches stop being an issue at low resolutions
as many photons cease to contribute to the render, effectively increasing the yield.

We are limited in the size of the hash table; a 16k by 16k one is already 4GB so that's probably the absolute limit. We have 4k by 4k at the moment
which is a healthy 256MB. When tracing 100k photons we have:

N = 100k
M = 16M

Expected number of collisions is:

N(N - 1) / (2M) = 312

If we want k collisions then we need:

N(N - 1) / (2M) = k

N(N - 1) = 2kM

~ N^2 = 2kM

N = sqrt(2kM)

So in early SPPM iterations it may be better to trace LESS photons per batch to reduce the likelihood of collisions, then in later iterations we
can ramp the photon batch size up because occasional collisions are less of a concern.

This all points to the fact that the hash grid is NOT a good data structure for early passes. It's amazing for later passes where the cell size is
so small it just works beautifully, but for large grid cells it is not even close to optimal.

One thing we could do to reduce collisions is to allow for more than 1 photon per cell, maybe 2 or 4. This increases the size of our hash table
considerably and makes querying more expensive, but it should dramatically reduce the number of collisions


M = 100k
N = 16M

M is definitely < N / log(N)

So the estimated maximum load will be:

log(N) / (log(N / M)) ~ 3

So allowing for up to 3 photons per cell would be a reasonable approach. How do we do that efficiently?

One option is to simply stratify the hash table. Instead of generating 100k photons, generate 20k at a time, five times with an instanced draw.
The gl_InstanceID is then used to index into a specific subset of the table, such that each cell consists of 5 consecutive slots written one after
the other. The likelihood of a collision has decreased significantly, however querying now takes 5 times as much work (although the cell is still
one consecutive block of memory, so queries should still remain pretty fast).

At least the photon generation phase is not any more expensive.

If we do this with the given scenario, AND reuse the same photon table so that we don't use 5x more memory, we get the following:

M = 20k
N = 16M / 5

log(N) / (log(N / M)) ~ 2.95

not a huge improvement. but if we use more memory, thereby keeping N = 16M, we have:

log(N) / (log(N / M)) ~ 2.48

This is probably not a winning strategy; we need to trade off way too much memory to get any substantial benefit...

The PROBLEM is multiple photons falling on the same area and overwriting one another.

One option is to simply... IGNORE them. As in, detect that two photons landed in the same spot, and treat them as if no photon had landed there.

Does this help? Probably not... I think the problem here is not so much a missed photon (which isn't actually a big deal; the visible point would
simply not update its radius and the photon would be caught on the next iteration). The real problem is two photons landing in the same spot, which
causes the visible points to only register 1 photon when all other nearby visible points registered 2 or more; this is probably what is introducing
the bias.

We should try with a stencil buffer approach which simply detects and rejects "tainted" cells where duplicate photons have been written to. Simply
ignore them, which should take care of everything; this is a quite low probability event anyway so the bias should be quite unnoticeable. Let's
try it out.

This is quite a simple change actually; we need:

 1. create a stencil buffer of the same size as the photon table
 2. clear the stencil buffer at the same time as the photon table
 3. set a stencil state which says:
    * increment the stencil buffer on a rasterized pixel
    * fail the stencil test if the stencil is not zero
 4. read the stencil buffer back at the same time as reading the photon table; if the stencil value is non-zero, disregard any photons in the cell
    (this might actually speed up the query at high grid cell resolutions since we can skip querying the photon table)


One possible issue here: the rasterization order may be unspecified. We can fix this by adding a deterministic, randomly generated depth value per
photon as well, but this is potentially expensive, we could skip it until it becomes a problem.

Idea: additively blend position, direction and throughput of photons in the photon table, and store a photon count as well.

When gathering:

  1. check length(direction) / photon count, if it's close to 1 all the photons were coming from the same direction, so assign a high weight
     to the throughput estimate (don't divide by photon count). If the length is too far below 1, reject the sample altogether.
       -> maybe we can use this as the "photon count" as a continuous value? not sure really
  2. use position / photon count to get the actual photon position

This will allow retaining multiple photons in the map if they all come from roughly the same direction, while rejecting them if they are unrelated
photons. Need to see how much bias this introduces, also this may only be useful early on in the render, and we should look into narrowing the grid
as well over time so that this becomes less useful.

This approach doesn't work and introduces too much bias. We will do this:

Let the user specify an expected specific photon density measure with units 1 / m^2. Call this D.

Then, given an average search radius Rs for the current SPPM pass, we can select a grid cell size C which minimizes the average cost of the
gather pass. For instance C = 2Rs or something similar. Given this, we only want one photon per cell at most, meaning we want:

C^3 * D * N = 1  =>  N = 1 / (C^2 * D)

This is the maximum number of photons we are allowed to fire in a given scatter-gather pass to avoid exhausting our hash table.

So for instance with a specific density of 10000 per cubic meter, and a grid cell size of 1mm, we get N = 100k

The basic idea is that if C is large, N must be low, whereas if C is low, N can be large.

If N is too low for the pass to be efficient, we can still parallelize by dividing the hash table into M sections, and doing M passes of N
photons, with each pass dropping a photon in a given section of the hash table. The gather pass simply scans the M "subcells" in each cell
to accumulate the photons. This is probably more efficient than doing multiple separate scatter-gather passes.

So, for each SPPM pass (one per frame) do the following:

 - select a target number of photons per pass Np
 - select grid cell size C based on the average search radius
 - pick the largest N we can, based on hash table size and user-provided photon density estimates
 - set M = Np / N, rounded to a power of two
 - do M (instanced) scatter passes, with each instance scattering into the corresponding hash subtable
 - do a gather pass, which reads M entries per cell

Initially we might start with M = 4 or 8 or similar, and as C decreases (because the search radius decreases) N will increase and M will
decrease, after some (small) number of passes we will have M = 1 and will be running at full speed. The scatter phase is always as fast as
possible, the gather phase is slow by a factor of M (approximately).

Selection of C
==============

Given the search radius Rs we simply pick a grid cell size which minimizes the amount of gather work required. At most we should want to look
up 4 cells for each visible point, which is possible if we set C = 2Rs. Then, given a visible point inside a grid cell, we only have to search
the neighbours that it is closest to (up to 4), and not all 26 neighbours.

Selection of Np
===============

There is an absolute limit for Np that is dependent on the hash table size. This is the point at which we expect a non-negligible number of
collisions. Fundamentally, a photon collision means all photons but the last one were wasted; too many photons and we are just wasting power
computing photons which are going to get overwritten later.

This needs proper analysis but for now we can say that Np must be <= 5% of the hash table capacity. So 16M entries => 800k photons max.

This also depends on the scene and maybe on the device, so we could just leave this as a configurable option together with the hash table
capacity. Just put some reasonable limits on it.

Selection of N
==============

This one is trickier. We need to make sure that, given the user-provided density estimates (which in turn depend on how the lights are sampled)
we don't consistently register more than one photon in a cell. Doubling the number of photons fired will double the expected number of photons
in each cell, so we want:

if N * D is photons / m^2, then the number of photons in C^2 will be N * D * C^2. We want N * D * C^2 = 1 photon, so N = 1 photon / (D * C^2)

N * C^2 / D = 1    =>    N = D / C^2   =>   N = D / (4 Rs^2)

Where C is the grid cell size, and D is a user-provided photon density, in units of photons / m^2. It's essentially a scaling factor which
determines the maximum photon density in the scene. If D is higher than the true photon density, we will fire too many photons and will register
more than one photon per cell, leading to bias. If D is lower than the true photon density, the hash table will be underutilized and we will
waste performance in the gather phase.

It's obvious that N must be proportional to 1 / C^2, since the smaller the grid cell size the more photons we can register in the grid per unit
area. We can define some proportionality factor which allows scaling the number of photons; it should be tweaked per-scene. If it's set too
high, bias will be introduced as too many photons are overwritten and artifacts will show up. If it's set too low, the hash table will be
underutilized and the gather phase will be slower than it needs to be.

[photons] = X [meters]^[-2]

so X is [photons] [meters]^2. not sure what the physical significance of this value is. Make X configurable, and it can be initialized to 1.

Selection of M
==============

M = Np / N

How does Rs decrease? Under the assumption that we get one photon per cell (or M photons per cell really) per pass, and C = 2Rs, we can expect
the search radius for each visible point to contain some number of cells.

Volume of the search radius: 4/3 pi Rs^3

Volume of a cell: 4/3 pi C^3 = 4/3 pi 8Rs^3

4/3 pi Rs^3 / 4/3 pi 8Rs^3 = 1 / 8

So we expect M / 8 photons per visible point on average. We need to be accurate here to ensure we don't reduce the radius too quickly.

TODO:

 1. make Np, the initial search radius and "X" configurable in the JSON.
DONE
 2. pick C = 2 Rs at every pass, for now don't reduce Rs.
DONE
 3. pick N and M correctly, and implement the instanced scatter-gather pass.
 4. maintain a total photon count to normalize the final result
DONE (sort of)
 5. reduce Rs, and adjust the gather fragment shader to either always be correct, or to always clamp the pixel's search radius to Rs

With some luck the gather with M > 1 will be quite fast and we can rely on just making M large to compensate for large initial search radii without
too much slowdown. Once we are past the first few iterations anyway Rs will be quite low and we can go at full speed again.

 => we can also cap M with some option so that if the first few passes are too slow we can simply use less photons


Need to implement the hash cell rows/cols now so that we can do multiple gather.

This involves passing the relevant row/col values to both shaders. The scatter pass uses gl_InstanceID to calculate the location to place the
photon in inside the cell:

gl_InstanceID % hash_cell_cols
gl_InstanceID / hash_cell_cols

The gather pass simply iterates over all of them; problem solved (kind of efficiently).

Need to solve edge artifacts... it's bad...

================

According to the PBR book, we have the following:

 * the division by Np pi r^2 does NOT happen in the rendering loop; it's done when calculating the photon radiance but does not
   feed back into the system.
 * direct and indirect lighting (in our case, the "no visible point" value and the visible point contributions) are handled separately

The photon radiance is simply computed as:

 tau / (Np * pi * radius^2)

Where tau is calculated during the update, and Np is presumably the number of photons in the current pass?


Everything seems to work great otherwise, need to put back the visible lighting component and we should be good.

The remaining tasks are:

 - putting back visible lighting (so apparently this needs to be treated as a separate quantity and normalized by the
   pass count rather than the total number of photons and should not be mixed with the photon radiance estimates)
DONE (need to check there are no edge artifacts...)
 - shrinking the pixel radius (for the grid) somehow
    -> this will allow us to trace more photons as the grid shrinks, quickly speeding up the render
       right now we are constrained to whatever the initial grid size is, forever
 - dynamic calculation of n and m
 - allow marking individual surfaces as photon receivers
    -> this allows saying that for some really glossy surfaces we should just trace rays to estimate radiance, instead of trying to
       estimate it with photons. specular surfaces will never be photon receivers, obviously
    -> this can just replace the current "allow_mis" flag; MIS is irrelevant with the direct SPPM we are doing
    -> at the moment all non-specular surfaces are receivers
 - investigate serious performance regressions in shader builds (this wouldn't be so bad if it didn't freeze the whole browser)
 - handle all bounces, not just one non-specular bounce...

Need to test the convergence as well to see if it looks the same as before.

Image gets darker... this is probably due to the search radius going down too fast and screwing up the estimates.
Just copying the search radius to the CPU and updating it every now and then is a definite option if we want to "be sure".
The logic for it shouldn't be too bad, though we do need to allocate a separate copy buffer and implement a fence object...
