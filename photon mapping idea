1. collect all "visible points" into a grid data-structure. each point has its own search radius
2. for each photon emitted from the lights, at every diffuse hit, find all visible point search radii in range and accumulate the photon
    -> grid structure probably works quite well here, is it possible to render to a 3D texture directly?
    -> render all visible point info into a (number of) texture(s) containing position, search radius, current value, etc..
    -> also store a 3D texture which simply indexes into that texture with 1 int, so it's a big 3D R32UI texture.
    -> this would be close to optimal for the accumulation stage because we can just do an instanced draw where the Ith instance will look
       at the Ith point in the cell or adjacent cells?

Can we do better?

Let's think about grids for a bit; how can we store this information efficiently on the GPU?

If we stored indices to the visible points in a linear texture, such that each "cell" in the 3D grid only stores an offset and length into that texture
exactly representing the list of visible points inside that cell, then:

 - if we know the maximum number of points in a cell, say M (hopefully )

This will become incredibly inefficient as the search radius gets smaller and less and less photons are picked up...

What about a big, fixed-size hash table? The hash table maps from tiny 3D squares to some index in a linear array.



Turn the problem around: generate the visible points, with no acceleration structure. Then generate a bunch of photons, put THOSE in an acceleration
structure, and then for each visible point (= pixel), gather all photons within the search radius. The advantage is that there is no scatter write
operation in this case, we can stay within the fragment shader, which simply accumulates photons within the search radius and (more importantly)
knows WHEN to stop, i.e. when all photons in range have been gathered!

The downside, of course, is that the photons now need to be in an acceleration structure, which may be undesirable. But with a "pass" system I think
it's equivalent.

We can use scatter to construct the acceleration structure maybe

If we use a fixed-size hash table, then each generated photon has a given coordinate, which in turn can map into some index into the linear array.
How do we avoid duplicates/collisions? Can always happen as photons can get arbitrarily close to one another, but... if this happens... we can just ignore
all but one of the photons and only lose a small % of performance. We certainly could not do this with visible points without introducing bias. The
question now becomes, how small does this "distance" need to be for photon collisions to be negligible?

 -> actually I think it still does introduce bias, but it should be fairly minimal. maybe not though?

One other thing is that (in general, probably), the # of photons per pass is less than the number of visible points.

We could also use randomness to our advantage here; allocate a big texture (e.g. 8192 x 8192) and say we have 200k photons per pass. Each photon is
mapped to its coordinate with a simple mapping formula. Then with a perfectly random set of photons we would expect an occupancy of...

200k / 8192^2 = 0.3%

of course in practice many photons will "bunch up" into specific locations in the scene.

what kind of operations do we need?

for each visible point:

 - find all the surrounding "grids" in which the contributing photons might lie (depending on grid resolution and visible point position)
 - look up the coordinate for that grid
 - if there is a photon in that grid, accumulate it

so the main issue is, what if there is more than one photon in the grid, what do we do then. the simplest approach is to simply discard it, and
only keep one photon, but this can lead to very high bias, we can't do that.

if we use a random hashing function, then we can "compress" very small grid cells into a small linear buffer, randomly "spreading" photons throughout
the space. Unfortunately the tradeoff is that for each visible point we must now check many (possibly incoherent) grid cells in the buffer, but
collisions are much less likely.



If we allow doing some work on the CPU then we can just quickly build a kd-tree.

Approach:

 1. generate the photons on the GPU
 2. read back the photon data and build a kd-tree of it <- STALL
 3. generate the visible points on the GPU, and query the kd-tree

how do we avoid the stall?? we could avoid it by doing something else in the meantime, like computing the direct lighting. alternatively we could
just start computing the visible points and storing them in a texture, then (when the kd-tree is ready) actually accumulate photon contributions.


GPU: compute visible points A
GPU: compute photons A
GPU: compute visible points B   CPU: read back photons A
GPU: compute photons B          CPU: write back photons 



Let's do this:

each frame:

 - CPU queues up a photon generation pass, and has the GPU read them back into a PBO
 - CPU queues up visible point generation + direct lighting pass
 - CPU reads back from the PBO and generates the kd-tree data (blocking)
 - CPU queues up photon accumulation pass using the kd-tree

the idea is that the PBO readback happens automatically, and we can hopefully build the kd-tree while the visible point generation pass is being
done in the GPU. that would hide the latency, but is it viable? can we generate a kd-tree for, say, 200k points in a few milliseconds? it _should_
be possible. The CPU side would become a bottleneck though

Alternate solutions without readback or CPU intervention involve:

 1. generating a bunch of photons, say 10 million
 2. for every 100k photons:
    - pick a random "projection plane" splitting the scene
    - project every photon onto that plane, into a quantized 2D texture; photons landing in the same texel are overwritten
    - for every visible point:
        - project visible point onto the plane, and find nearby photons using the 2D plane

The 2D plane should be high resolution enough relative to the number of photons so that 



We could construct a quad-tree structure on the GPU for the 2D plane using mipmaps; this would be reasonably easy to do I think.

Could we project the photons onto two (three?) planes at once, and somehow pick the right one? The idea being (hopefully) that
photons that would project identically on one plane, would project differently in another plane, unless they are actually really
close to one another. So in theory a photon is only close to the visible point if it projects closely in all three dimensions.

This is an idea, let's consider it... it'd be great to be able to stay purely on the GPU for sure.

So now the idea is, we get these three planes with photons projected onto them texel-by-texel (the planes need to be high-resolution
to ensure nothing is missed). Then generate a quad-tree using the mipmaps, by rendering into each layer using the following rule:
 - write a 1 if any of the four lower texels are occupied by a photon

 Then when generating visible points, for each plane:

 - start at the highest mipmap
 - recurse into specific texels of the lower layer if they are 1 and the entire texel range described is in range
 - accumulate photons from the lowest mip

The catch: cannot accumulate identical photons coming from different planes multiple times. need some way to reject them

With two planes:

 * each location in 3D space is uniquely determined the combined (x1, y1), (x2, y2) location that the point is projected on in both planes.
   if the point is (x, y, z) then (x1, y1) = (x, y) and (x2, y2) = (z, y)

 * are there still cases where photons are nearby to a point P in both planes, yet physically far away? I don't think so. so it should work?

Given a 1 meter by 1 meter scene, 4096x4096 planes give a precision of 250um, pretty decent. This kind of falls apart with larger scenes
where the maximum photon precision becomes very low, meaning that lots of photons will be rejected. If we use 32-bit ints to address the
photon data, then 4096x4096 would only be 64MB, so we could go up to 16K x 16K easily (= 1GB) and possibly higher. These naturally give
much better precision and allow larger scenes (e.g. 100 meters by 100 meters at 16K is a precision of 6mm)

(although it's worth pointing out that if lighting is uniform in these larger scenes, the photon density would naturally decrease at the same
 rate such that it wouldn't be too big a deal)

If we only had a way to deal with these photons, we'd be set. The problem is we have no way to detect duplicate photons, as they are just generated
in parallel. One thing we might be able to do though, is detect when it happens, using the stencil buffer, and simply mask out those "bad" photon
generations in a separate pass. Of course the issue now is that we are straight up rejecting these particular events, but they do need to be
handled. Having multiple photons from different sources hit the same surface at approximately the same location is kind of important. Is there some
systematic way we can handle this? We could also use a depth buffer with a custom gl_FragDepth to give different priorities to each photon, so that
in the event of two photons hitting exactly the same spot, the more significant of the two would get to stay.

Actually I just realized we cannot avoid this, because for e.g. caustics the intensity of all photons is pretty much the same, and the caustic
brightness is simply determined by photon density over the surface, so ALL photon hits must be recorded to get a good estimate.





What if:

 - we stored each photon inside a hashed array, so that even nearby photons get put in wildly different locations in the array; as long as the
   number of array elements is vastly higher than the number of traced photons (easy) then we are good and there will be no (systematic)
   collisions (a few random bad-luck collisions are okay, they won't be significant and should average out).
 - we _also_ build an octree of quantized "occupied" cells (occupied by photons) in the scene's volume; the goal of this is to 




Can we transform-feedback into the same buffer, overlapping?

<initial photon data> <first bounce photon data> <second bounce photon data> ...

then read it all back on the CPU and generate the data? no, this sucks...

<source photon data>
<output photon data>

Alternatively, just use a texture to hold this state. The main state we're interested in is the new photon direction, and its position, which we
can just read from the previously rendered textures. So, store a few textures:


 photon_position_texture_a: RGBA32F
 photon_position_texture_b: RGBA32F
 photon_record_texture_2: RGBA32F (recorded photon position)
 photon_record_texture: RGBA32F   (incident direction + incident throughput, as RG16F + RGB32F packed)
 photon_direction_texture: RGBA32F


So the algorithm would be:

 1. render a number of first-bounce photons;
     - incident positions, directions and throughputs go directly into (some region of) the two record_texture
     - new and positions directions after the first bounce are fed back into a transform feedback buffer
 2. render a number of second-bounce photons, using the photon position/direction/throughputs in the buffer
     - incident positions, directions and throughputs go directly into (some region of) the two record_texture
     - etc...

Another option is to store everything inside the vertex buffers. Then, we copy that to the CPU which generates a kd-tree for it
and uploads it into a texture. We can copy the data progressively to a separate buffer, and then only read the POSITION data back
to the CPU to build the kd-tree. The photon position/direction/throughput data gets copied directly to a GPU texture. Then that
texture together with the kd-tree is used to query the nearby photons during visible point construction.

In this case what do we need? We should test if we can use overlapping ranges of a buffer for the transform feedback buffer, then we
only need two feedback buffers (one for the positions, and one for the rest) and no rasterization is required during that stage; at
every step (every bounce) we copy the position data into a copy buffer on the CPU, and we copy the two buffers into part of two photon
textures which we store the data in.

When we've got all our photons, we read the position buffer into the CPU, construct the kd-tree and upload it to a texture. Then do
the visible point construction pass, updating the pixels, then update the visible point search radius and so on and repeat for the
next pass.

So:

 - 2 transform feedback buffers
 - 2 photon textures
 - 1 copy-read buffer
 - 1 kd-tree texture




Kd-tree sucks and will never be viable; we need to stay on the GPU. uniform grid is probably the only way in WebGL.

The main problem with the grid is we can't handle photon collisions well; photons that land in the same grid cell. The only way to handle this
is to store them together; discarding the photon will introduce catastrophic bias in the photon distribution.

But what if we could implement an actual hash-table-like structure? Given the "old" hash table, and a set of photons, we produce a "new" hash table,
which works by:

 1. figuring out which grid cell the photon belongs to
 2. reading the old hash table to find the first unoccupied slot
 3. writing out the new photon in the appropriate location

This loses some performance



How do we store this? Ideally we have one photon per pixel or else things become complicated; we can have up to 16 bytes per pixel maximum.

Can we condense a single photon in 16 bytes?

Incident direction in 2 bytes, incident throughput in an RGBE value? That leaves 10 bytes for the position... not ideal.

If we pack the position in half-floats it would fit well, but we may run into serious accuracy issues there... although it might be okay.

So:

R = position X + position Y
G = position Z + throughput R
B = throughput G + throughput B
A = direction X + direction Y (sign of Z in sign of throughput R)

 -> could compress the position by storing it referred from the center of the grid cell or whatever?

Main problem here is how to update the hash table, and how often to do it; we can't "update" the hash table reliably like this because duplicate
photons in the same batch will overwrite one another. One option could be a compaction pass but this is problematic in general.

We can reduce the chance of collisions by using smaller batches but then the performance decreases.

We could separate the passes, to render different lights; for "concentrated" lights which are likely to emit many photons in the same location we can
use a separate hash table. 

Another option is to go full basic photon-mapping on this, and just generate millions of photons, store them all in a kd-tree on the CPU ONCE, and
then reuse this map forever. This has a high cost however, and is not really adaptive.

Another option is to still use photon-mapping, but store them all in a uniform grid with very high resolution (to reduce collisions).

Couldn't we argue though that duplicate photons being lost is not really important in the early stages of SPPM? we lose some information and introduce
extra bias, but as the radius becomes lower these photons start becoming more likely to be counted... let's do this I think, it should work.
